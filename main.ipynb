{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEGAN without the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original SEGAN paper: https://arxiv.org/pdf/1703.09452.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio import transforms\n",
    "from data import SpeechDataset\n",
    "import time\n",
    "from model import Autoencoder\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from pypesq import pesq\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 512\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = filter(lambda x: re.search(\"^seae_epoch_\\d+\\.pth$\", x), os.listdir('models'))\n",
    "epochs = map(lambda x: int(re.search(\"^seae_epoch_(\\d+)\\.pth$\", x)[1]), epochs)\n",
    "last_epoch = max(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = f'models/seae_epoch_{last_epoch}.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to run the training loop, download the OpenSLR12 dataset (http://www.openslr.org/12/), convert all .flac files to .wav and copy to 'data/clean/open_slr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpeechDataset('data/clean/360/', 'data/noise/', window_size=16384, overlap=50, snr=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd853fa6404445c8faa4bf7089c7edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d69222f31144b7dacd28816a04054eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/dan/www/sound/model.py(88)forward()\n",
      "-> attn_h = self.attn_h(c_11)\n",
      "(Pdb) attn_f * attn_g\n",
      "tensor([[[-1.7686e-05, -1.6992e-05, -1.9721e-05,  ...,  1.0150e-05,\n",
      "           1.2156e-05, -1.1872e-05],\n",
      "         [-3.9465e-04, -3.9290e-04, -3.3576e-04,  ..., -3.9160e-04,\n",
      "          -4.2370e-04, -4.5246e-04],\n",
      "         [-4.8166e-05, -7.9717e-05, -9.7985e-05,  ..., -5.6395e-05,\n",
      "          -2.3661e-05, -8.8948e-06],\n",
      "         ...,\n",
      "         [-5.1417e-04, -4.5519e-04, -4.1274e-04,  ..., -2.8816e-04,\n",
      "          -4.0467e-04, -4.5172e-04],\n",
      "         [-5.5776e-05, -2.9450e-05, -2.3937e-05,  ..., -3.7759e-05,\n",
      "          -2.4457e-05, -3.9439e-05],\n",
      "         [ 6.3374e-04,  7.1168e-04,  6.7480e-04,  ...,  5.2331e-04,\n",
      "           5.3777e-04,  4.9608e-04]],\n",
      "\n",
      "        [[-1.7755e-05, -1.6998e-05, -1.9801e-05,  ...,  1.0261e-05,\n",
      "           1.2017e-05, -1.1796e-05],\n",
      "         [-3.9532e-04, -3.9194e-04, -3.3505e-04,  ..., -3.9220e-04,\n",
      "          -4.2350e-04, -4.5180e-04],\n",
      "         [-4.8370e-05, -7.9316e-05, -9.7914e-05,  ..., -5.5490e-05,\n",
      "          -2.3390e-05, -8.8825e-06],\n",
      "         ...,\n",
      "         [-5.1356e-04, -4.5450e-04, -4.1210e-04,  ..., -2.8792e-04,\n",
      "          -4.0469e-04, -4.5188e-04],\n",
      "         [-5.5452e-05, -2.9479e-05, -2.3811e-05,  ..., -3.7748e-05,\n",
      "          -2.4102e-05, -3.9429e-05],\n",
      "         [ 6.3511e-04,  7.1189e-04,  6.7429e-04,  ...,  5.2486e-04,\n",
      "           5.3712e-04,  4.9649e-04]],\n",
      "\n",
      "        [[-1.7634e-05, -1.6839e-05, -1.9750e-05,  ...,  1.0144e-05,\n",
      "           1.2115e-05, -1.1910e-05],\n",
      "         [-3.9492e-04, -3.9292e-04, -3.3563e-04,  ..., -3.9181e-04,\n",
      "          -4.2379e-04, -4.5223e-04],\n",
      "         [-4.8100e-05, -7.9561e-05, -9.8050e-05,  ..., -5.6155e-05,\n",
      "          -2.3697e-05, -8.5647e-06],\n",
      "         ...,\n",
      "         [-5.1379e-04, -4.5534e-04, -4.1271e-04,  ..., -2.8806e-04,\n",
      "          -4.0459e-04, -4.5215e-04],\n",
      "         [-5.5646e-05, -2.9438e-05, -2.4052e-05,  ..., -3.7969e-05,\n",
      "          -2.4395e-05, -3.9487e-05],\n",
      "         [ 6.3391e-04,  7.1164e-04,  6.7525e-04,  ...,  5.2389e-04,\n",
      "           5.3801e-04,  4.9645e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.7632e-05, -1.6930e-05, -1.9916e-05,  ...,  1.0216e-05,\n",
      "           1.2135e-05, -1.1716e-05],\n",
      "         [-3.9399e-04, -3.9267e-04, -3.3436e-04,  ..., -3.9156e-04,\n",
      "          -4.2291e-04, -4.5171e-04],\n",
      "         [-4.8065e-05, -7.9361e-05, -9.8039e-05,  ..., -5.6160e-05,\n",
      "          -2.4001e-05, -8.8257e-06],\n",
      "         ...,\n",
      "         [-5.1452e-04, -4.5529e-04, -4.1165e-04,  ..., -2.8891e-04,\n",
      "          -4.0440e-04, -4.5227e-04],\n",
      "         [-5.5909e-05, -2.9430e-05, -2.3825e-05,  ..., -3.8184e-05,\n",
      "          -2.4093e-05, -3.9460e-05],\n",
      "         [ 6.3441e-04,  7.1081e-04,  6.7451e-04,  ...,  5.2467e-04,\n",
      "           5.3870e-04,  4.9575e-04]],\n",
      "\n",
      "        [[-1.7688e-05, -1.6936e-05, -1.9717e-05,  ...,  1.0366e-05,\n",
      "           1.2260e-05, -1.1695e-05],\n",
      "         [-3.9461e-04, -3.9308e-04, -3.3488e-04,  ..., -3.9215e-04,\n",
      "          -4.2355e-04, -4.5264e-04],\n",
      "         [-4.8105e-05, -7.9674e-05, -9.8388e-05,  ..., -5.5844e-05,\n",
      "          -2.3533e-05, -8.4163e-06],\n",
      "         ...,\n",
      "         [-5.1364e-04, -4.5527e-04, -4.1204e-04,  ..., -2.8819e-04,\n",
      "          -4.0418e-04, -4.5239e-04],\n",
      "         [-5.5190e-05, -2.9417e-05, -2.3888e-05,  ..., -3.8465e-05,\n",
      "          -2.4227e-05, -3.9594e-05],\n",
      "         [ 6.3484e-04,  7.1201e-04,  6.7512e-04,  ...,  5.2389e-04,\n",
      "           5.3800e-04,  4.9638e-04]],\n",
      "\n",
      "        [[-1.7544e-05, -1.7056e-05, -1.9913e-05,  ...,  1.0196e-05,\n",
      "           1.2156e-05, -1.1793e-05],\n",
      "         [-3.9479e-04, -3.9240e-04, -3.3508e-04,  ..., -3.9105e-04,\n",
      "          -4.2312e-04, -4.5218e-04],\n",
      "         [-4.7850e-05, -7.9898e-05, -9.7424e-05,  ..., -5.5601e-05,\n",
      "          -2.3583e-05, -8.6586e-06],\n",
      "         ...,\n",
      "         [-5.1427e-04, -4.5548e-04, -4.1120e-04,  ..., -2.8807e-04,\n",
      "          -4.0446e-04, -4.5225e-04],\n",
      "         [-5.5512e-05, -2.9198e-05, -2.3928e-05,  ..., -3.8302e-05,\n",
      "          -2.4167e-05, -3.9557e-05],\n",
      "         [ 6.3320e-04,  7.1110e-04,  6.7469e-04,  ...,  5.2479e-04,\n",
      "           5.3752e-04,  4.9569e-04]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "(Pdb) attn_f.shape\n",
      "torch.Size([512, 1024, 8])\n",
      "(Pdb) (attn_f * attn_g).shape\n",
      "torch.Size([512, 1024, 8])\n",
      "(Pdb) torch.nn.softmax(attn_f * attn_g).shape\n",
      "*** AttributeError: module 'torch.nn' has no attribute 'softmax'\n",
      "(Pdb) nn.softmax(attn_f * attn_g).shape\n",
      "*** AttributeError: module 'torch.nn' has no attribute 'softmax'\n",
      "(Pdb) nn.Softmax(attn_f * attn_g).shape\n",
      "*** AttributeError: 'Softmax' object has no attribute 'shape'\n",
      "(Pdb) torch.nn.softmax(attn_f * attn_g)\n",
      "*** AttributeError: module 'torch.nn' has no attribute 'softmax'\n",
      "(Pdb) torch.nn.Softmax(attn_f * attn_g)\n",
      "Softmax(\n",
      "  dim=tensor([[[-1.7686e-05, -1.6992e-05, -1.9721e-05,  ...,  1.0150e-05,\n",
      "             1.2156e-05, -1.1872e-05],\n",
      "           [-3.9465e-04, -3.9290e-04, -3.3576e-04,  ..., -3.9160e-04,\n",
      "            -4.2370e-04, -4.5246e-04],\n",
      "           [-4.8166e-05, -7.9717e-05, -9.7985e-05,  ..., -5.6395e-05,\n",
      "            -2.3661e-05, -8.8948e-06],\n",
      "           ...,\n",
      "           [-5.1417e-04, -4.5519e-04, -4.1274e-04,  ..., -2.8816e-04,\n",
      "            -4.0467e-04, -4.5172e-04],\n",
      "           [-5.5776e-05, -2.9450e-05, -2.3937e-05,  ..., -3.7759e-05,\n",
      "            -2.4457e-05, -3.9439e-05],\n",
      "           [ 6.3374e-04,  7.1168e-04,  6.7480e-04,  ...,  5.2331e-04,\n",
      "             5.3777e-04,  4.9608e-04]],\n",
      "  \n",
      "          [[-1.7755e-05, -1.6998e-05, -1.9801e-05,  ...,  1.0261e-05,\n",
      "             1.2017e-05, -1.1796e-05],\n",
      "           [-3.9532e-04, -3.9194e-04, -3.3505e-04,  ..., -3.9220e-04,\n",
      "            -4.2350e-04, -4.5180e-04],\n",
      "           [-4.8370e-05, -7.9316e-05, -9.7914e-05,  ..., -5.5490e-05,\n",
      "            -2.3390e-05, -8.8825e-06],\n",
      "           ...,\n",
      "           [-5.1356e-04, -4.5450e-04, -4.1210e-04,  ..., -2.8792e-04,\n",
      "            -4.0469e-04, -4.5188e-04],\n",
      "           [-5.5452e-05, -2.9479e-05, -2.3811e-05,  ..., -3.7748e-05,\n",
      "            -2.4102e-05, -3.9429e-05],\n",
      "           [ 6.3511e-04,  7.1189e-04,  6.7429e-04,  ...,  5.2486e-04,\n",
      "             5.3712e-04,  4.9649e-04]],\n",
      "  \n",
      "          [[-1.7634e-05, -1.6839e-05, -1.9750e-05,  ...,  1.0144e-05,\n",
      "             1.2115e-05, -1.1910e-05],\n",
      "           [-3.9492e-04, -3.9292e-04, -3.3563e-04,  ..., -3.9181e-04,\n",
      "            -4.2379e-04, -4.5223e-04],\n",
      "           [-4.8100e-05, -7.9561e-05, -9.8050e-05,  ..., -5.6155e-05,\n",
      "            -2.3697e-05, -8.5647e-06],\n",
      "           ...,\n",
      "           [-5.1379e-04, -4.5534e-04, -4.1271e-04,  ..., -2.8806e-04,\n",
      "            -4.0459e-04, -4.5215e-04],\n",
      "           [-5.5646e-05, -2.9438e-05, -2.4052e-05,  ..., -3.7969e-05,\n",
      "            -2.4395e-05, -3.9487e-05],\n",
      "           [ 6.3391e-04,  7.1164e-04,  6.7525e-04,  ...,  5.2389e-04,\n",
      "             5.3801e-04,  4.9645e-04]],\n",
      "  \n",
      "          ...,\n",
      "  \n",
      "          [[-1.7632e-05, -1.6930e-05, -1.9916e-05,  ...,  1.0216e-05,\n",
      "             1.2135e-05, -1.1716e-05],\n",
      "           [-3.9399e-04, -3.9267e-04, -3.3436e-04,  ..., -3.9156e-04,\n",
      "            -4.2291e-04, -4.5171e-04],\n",
      "           [-4.8065e-05, -7.9361e-05, -9.8039e-05,  ..., -5.6160e-05,\n",
      "            -2.4001e-05, -8.8257e-06],\n",
      "           ...,\n",
      "           [-5.1452e-04, -4.5529e-04, -4.1165e-04,  ..., -2.8891e-04,\n",
      "            -4.0440e-04, -4.5227e-04],\n",
      "           [-5.5909e-05, -2.9430e-05, -2.3825e-05,  ..., -3.8184e-05,\n",
      "            -2.4093e-05, -3.9460e-05],\n",
      "           [ 6.3441e-04,  7.1081e-04,  6.7451e-04,  ...,  5.2467e-04,\n",
      "             5.3870e-04,  4.9575e-04]],\n",
      "  \n",
      "          [[-1.7688e-05, -1.6936e-05, -1.9717e-05,  ...,  1.0366e-05,\n",
      "             1.2260e-05, -1.1695e-05],\n",
      "           [-3.9461e-04, -3.9308e-04, -3.3488e-04,  ..., -3.9215e-04,\n",
      "            -4.2355e-04, -4.5264e-04],\n",
      "           [-4.8105e-05, -7.9674e-05, -9.8388e-05,  ..., -5.5844e-05,\n",
      "            -2.3533e-05, -8.4163e-06],\n",
      "           ...,\n",
      "           [-5.1364e-04, -4.5527e-04, -4.1204e-04,  ..., -2.8819e-04,\n",
      "            -4.0418e-04, -4.5239e-04],\n",
      "           [-5.5190e-05, -2.9417e-05, -2.3888e-05,  ..., -3.8465e-05,\n",
      "            -2.4227e-05, -3.9594e-05],\n",
      "           [ 6.3484e-04,  7.1201e-04,  6.7512e-04,  ...,  5.2389e-04,\n",
      "             5.3800e-04,  4.9638e-04]],\n",
      "  \n",
      "          [[-1.7544e-05, -1.7056e-05, -1.9913e-05,  ...,  1.0196e-05,\n",
      "             1.2156e-05, -1.1793e-05],\n",
      "           [-3.9479e-04, -3.9240e-04, -3.3508e-04,  ..., -3.9105e-04,\n",
      "            -4.2312e-04, -4.5218e-04],\n",
      "           [-4.7850e-05, -7.9898e-05, -9.7424e-05,  ..., -5.5601e-05,\n",
      "            -2.3583e-05, -8.6586e-06],\n",
      "           ...,\n",
      "           [-5.1427e-04, -4.5548e-04, -4.1120e-04,  ..., -2.8807e-04,\n",
      "            -4.0446e-04, -4.5225e-04],\n",
      "           [-5.5512e-05, -2.9198e-05, -2.3928e-05,  ..., -3.8302e-05,\n",
      "            -2.4167e-05, -3.9557e-05],\n",
      "           [ 6.3320e-04,  7.1110e-04,  6.7469e-04,  ...,  5.2479e-04,\n",
      "             5.3752e-04,  4.9569e-04]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pdb) torch.nn.softmax(attn_f * attn_g)()\n",
      "*** AttributeError: module 'torch.nn' has no attribute 'softmax'\n",
      "(Pdb) torch.nn.Softmax(attn_f * attn_g)\n",
      "Softmax(\n",
      "  dim=tensor([[[-1.7686e-05, -1.6992e-05, -1.9721e-05,  ...,  1.0150e-05,\n",
      "             1.2156e-05, -1.1872e-05],\n",
      "           [-3.9465e-04, -3.9290e-04, -3.3576e-04,  ..., -3.9160e-04,\n",
      "            -4.2370e-04, -4.5246e-04],\n",
      "           [-4.8166e-05, -7.9717e-05, -9.7985e-05,  ..., -5.6395e-05,\n",
      "            -2.3661e-05, -8.8948e-06],\n",
      "           ...,\n",
      "           [-5.1417e-04, -4.5519e-04, -4.1274e-04,  ..., -2.8816e-04,\n",
      "            -4.0467e-04, -4.5172e-04],\n",
      "           [-5.5776e-05, -2.9450e-05, -2.3937e-05,  ..., -3.7759e-05,\n",
      "            -2.4457e-05, -3.9439e-05],\n",
      "           [ 6.3374e-04,  7.1168e-04,  6.7480e-04,  ...,  5.2331e-04,\n",
      "             5.3777e-04,  4.9608e-04]],\n",
      "  \n",
      "          [[-1.7755e-05, -1.6998e-05, -1.9801e-05,  ...,  1.0261e-05,\n",
      "             1.2017e-05, -1.1796e-05],\n",
      "           [-3.9532e-04, -3.9194e-04, -3.3505e-04,  ..., -3.9220e-04,\n",
      "            -4.2350e-04, -4.5180e-04],\n",
      "           [-4.8370e-05, -7.9316e-05, -9.7914e-05,  ..., -5.5490e-05,\n",
      "            -2.3390e-05, -8.8825e-06],\n",
      "           ...,\n",
      "           [-5.1356e-04, -4.5450e-04, -4.1210e-04,  ..., -2.8792e-04,\n",
      "            -4.0469e-04, -4.5188e-04],\n",
      "           [-5.5452e-05, -2.9479e-05, -2.3811e-05,  ..., -3.7748e-05,\n",
      "            -2.4102e-05, -3.9429e-05],\n",
      "           [ 6.3511e-04,  7.1189e-04,  6.7429e-04,  ...,  5.2486e-04,\n",
      "             5.3712e-04,  4.9649e-04]],\n",
      "  \n",
      "          [[-1.7634e-05, -1.6839e-05, -1.9750e-05,  ...,  1.0144e-05,\n",
      "             1.2115e-05, -1.1910e-05],\n",
      "           [-3.9492e-04, -3.9292e-04, -3.3563e-04,  ..., -3.9181e-04,\n",
      "            -4.2379e-04, -4.5223e-04],\n",
      "           [-4.8100e-05, -7.9561e-05, -9.8050e-05,  ..., -5.6155e-05,\n",
      "            -2.3697e-05, -8.5647e-06],\n",
      "           ...,\n",
      "           [-5.1379e-04, -4.5534e-04, -4.1271e-04,  ..., -2.8806e-04,\n",
      "            -4.0459e-04, -4.5215e-04],\n",
      "           [-5.5646e-05, -2.9438e-05, -2.4052e-05,  ..., -3.7969e-05,\n",
      "            -2.4395e-05, -3.9487e-05],\n",
      "           [ 6.3391e-04,  7.1164e-04,  6.7525e-04,  ...,  5.2389e-04,\n",
      "             5.3801e-04,  4.9645e-04]],\n",
      "  \n",
      "          ...,\n",
      "  \n",
      "          [[-1.7632e-05, -1.6930e-05, -1.9916e-05,  ...,  1.0216e-05,\n",
      "             1.2135e-05, -1.1716e-05],\n",
      "           [-3.9399e-04, -3.9267e-04, -3.3436e-04,  ..., -3.9156e-04,\n",
      "            -4.2291e-04, -4.5171e-04],\n",
      "           [-4.8065e-05, -7.9361e-05, -9.8039e-05,  ..., -5.6160e-05,\n",
      "            -2.4001e-05, -8.8257e-06],\n",
      "           ...,\n",
      "           [-5.1452e-04, -4.5529e-04, -4.1165e-04,  ..., -2.8891e-04,\n",
      "            -4.0440e-04, -4.5227e-04],\n",
      "           [-5.5909e-05, -2.9430e-05, -2.3825e-05,  ..., -3.8184e-05,\n",
      "            -2.4093e-05, -3.9460e-05],\n",
      "           [ 6.3441e-04,  7.1081e-04,  6.7451e-04,  ...,  5.2467e-04,\n",
      "             5.3870e-04,  4.9575e-04]],\n",
      "  \n",
      "          [[-1.7688e-05, -1.6936e-05, -1.9717e-05,  ...,  1.0366e-05,\n",
      "             1.2260e-05, -1.1695e-05],\n",
      "           [-3.9461e-04, -3.9308e-04, -3.3488e-04,  ..., -3.9215e-04,\n",
      "            -4.2355e-04, -4.5264e-04],\n",
      "           [-4.8105e-05, -7.9674e-05, -9.8388e-05,  ..., -5.5844e-05,\n",
      "            -2.3533e-05, -8.4163e-06],\n",
      "           ...,\n",
      "           [-5.1364e-04, -4.5527e-04, -4.1204e-04,  ..., -2.8819e-04,\n",
      "            -4.0418e-04, -4.5239e-04],\n",
      "           [-5.5190e-05, -2.9417e-05, -2.3888e-05,  ..., -3.8465e-05,\n",
      "            -2.4227e-05, -3.9594e-05],\n",
      "           [ 6.3484e-04,  7.1201e-04,  6.7512e-04,  ...,  5.2389e-04,\n",
      "             5.3800e-04,  4.9638e-04]],\n",
      "  \n",
      "          [[-1.7544e-05, -1.7056e-05, -1.9913e-05,  ...,  1.0196e-05,\n",
      "             1.2156e-05, -1.1793e-05],\n",
      "           [-3.9479e-04, -3.9240e-04, -3.3508e-04,  ..., -3.9105e-04,\n",
      "            -4.2312e-04, -4.5218e-04],\n",
      "           [-4.7850e-05, -7.9898e-05, -9.7424e-05,  ..., -5.5601e-05,\n",
      "            -2.3583e-05, -8.6586e-06],\n",
      "           ...,\n",
      "           [-5.1427e-04, -4.5548e-04, -4.1120e-04,  ..., -2.8807e-04,\n",
      "            -4.0446e-04, -4.5225e-04],\n",
      "           [-5.5512e-05, -2.9198e-05, -2.3928e-05,  ..., -3.8302e-05,\n",
      "            -2.4167e-05, -3.9557e-05],\n",
      "           [ 6.3320e-04,  7.1110e-04,  6.7469e-04,  ...,  5.2479e-04,\n",
      "             5.3752e-04,  4.9569e-04]]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      ")\n",
      "(Pdb) torch.nn.Softmax(attn_f * attn_g)()\n",
      "*** TypeError: forward() missing 1 required positional argument: 'input'\n",
      "(Pdb) torch.nn.Softmax()(attn_f * attn_g)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/www/sound/model.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  import torch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         ...,\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020]],\n",
      "\n",
      "        [[0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         ...,\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020]],\n",
      "\n",
      "        [[0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         ...,\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         ...,\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020]],\n",
      "\n",
      "        [[0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         ...,\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020]],\n",
      "\n",
      "        [[0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         ...,\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020],\n",
      "         [0.0020, 0.0020, 0.0020,  ..., 0.0020, 0.0020, 0.0020]]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "(Pdb) torch.nn.Softmax(dim=1)(attn_f * attn_g)\n",
      "tensor([[[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         ...,\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]],\n",
      "\n",
      "        [[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         ...,\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]],\n",
      "\n",
      "        [[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         ...,\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         ...,\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]],\n",
      "\n",
      "        [[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         ...,\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]],\n",
      "\n",
      "        [[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         ...,\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],\n",
      "         [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "(Pdb) attn_combine = torch.nn.Softmax(dim=1)(attn_f * attn_g)\n",
      "(Pdb) attn_combine.shape\n",
      "torch.Size([512, 1024, 8])\n",
      "(Pdb)  attn_h = self.attn_h(c_11)\n",
      "(Pdb) attn_combine * attn_h\n",
      "tensor([[[ 1.6577e-05,  1.5518e-05,  1.4935e-05,  ...,  1.5338e-05,\n",
      "           1.5886e-05,  1.6437e-05],\n",
      "         [-1.8628e-06, -1.1819e-06,  1.2424e-06,  ...,  2.0853e-06,\n",
      "           1.6088e-06,  7.4526e-07],\n",
      "         [ 2.1462e-06,  1.9274e-06,  3.8568e-06,  ...,  4.0920e-06,\n",
      "           4.1889e-06,  3.9302e-06],\n",
      "         ...,\n",
      "         [-8.3813e-06, -8.2978e-06, -7.0984e-06,  ..., -8.1265e-06,\n",
      "          -9.7729e-06, -1.0923e-05],\n",
      "         [-1.3243e-05, -1.3340e-05, -1.2682e-05,  ..., -8.3498e-06,\n",
      "          -6.9504e-06, -8.8470e-06],\n",
      "         [-7.8493e-06, -6.3101e-06, -6.1467e-06,  ..., -2.9140e-06,\n",
      "          -1.9734e-06,  9.6475e-07]],\n",
      "\n",
      "        [[ 1.6565e-05,  1.5518e-05,  1.4913e-05,  ...,  1.5319e-05,\n",
      "           1.5898e-05,  1.6418e-05],\n",
      "         [-1.8764e-06, -1.1983e-06,  1.2727e-06,  ...,  2.0805e-06,\n",
      "           1.5960e-06,  7.3437e-07],\n",
      "         [ 2.1590e-06,  1.9311e-06,  3.8416e-06,  ...,  4.1049e-06,\n",
      "           4.2303e-06,  3.9777e-06],\n",
      "         ...,\n",
      "         [-8.3882e-06, -8.3016e-06, -7.1212e-06,  ..., -8.1113e-06,\n",
      "          -9.7943e-06, -1.0938e-05],\n",
      "         [-1.3243e-05, -1.3344e-05, -1.2659e-05,  ..., -8.3776e-06,\n",
      "          -6.9765e-06, -8.8537e-06],\n",
      "         [-7.8608e-06, -6.3184e-06, -6.1393e-06,  ..., -2.8998e-06,\n",
      "          -1.9414e-06,  9.8406e-07]],\n",
      "\n",
      "        [[ 1.6559e-05,  1.5510e-05,  1.4935e-05,  ...,  1.5318e-05,\n",
      "           1.5893e-05,  1.6430e-05],\n",
      "         [-1.8522e-06, -1.1948e-06,  1.2479e-06,  ...,  2.0919e-06,\n",
      "           1.6080e-06,  7.3652e-07],\n",
      "         [ 2.1389e-06,  1.9216e-06,  3.8537e-06,  ...,  4.0932e-06,\n",
      "           4.2101e-06,  3.9413e-06],\n",
      "         ...,\n",
      "         [-8.3864e-06, -8.2990e-06, -7.1053e-06,  ..., -8.1287e-06,\n",
      "          -9.7791e-06, -1.0910e-05],\n",
      "         [-1.3245e-05, -1.3342e-05, -1.2681e-05,  ..., -8.3445e-06,\n",
      "          -6.9539e-06, -8.8266e-06],\n",
      "         [-7.8569e-06, -6.3198e-06, -6.1424e-06,  ..., -2.9142e-06,\n",
      "          -1.9958e-06,  9.6334e-07]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.6533e-05,  1.5515e-05,  1.4962e-05,  ...,  1.5329e-05,\n",
      "           1.5892e-05,  1.6461e-05],\n",
      "         [-1.8591e-06, -1.2057e-06,  1.2408e-06,  ...,  2.0865e-06,\n",
      "           1.6176e-06,  7.7816e-07],\n",
      "         [ 2.1367e-06,  1.9078e-06,  3.8226e-06,  ...,  4.0697e-06,\n",
      "           4.2117e-06,  3.9421e-06],\n",
      "         ...,\n",
      "         [-8.4070e-06, -8.3044e-06, -7.1247e-06,  ..., -8.1397e-06,\n",
      "          -9.7461e-06, -1.0893e-05],\n",
      "         [-1.3251e-05, -1.3369e-05, -1.2688e-05,  ..., -8.3072e-06,\n",
      "          -6.9604e-06, -8.8448e-06],\n",
      "         [-7.8795e-06, -6.3423e-06, -6.1626e-06,  ..., -2.9011e-06,\n",
      "          -1.9449e-06,  9.8518e-07]],\n",
      "\n",
      "        [[ 1.6530e-05,  1.5492e-05,  1.4928e-05,  ...,  1.5313e-05,\n",
      "           1.5878e-05,  1.6446e-05],\n",
      "         [-1.8936e-06, -1.2128e-06,  1.2585e-06,  ...,  2.0629e-06,\n",
      "           1.6108e-06,  7.3866e-07],\n",
      "         [ 2.1363e-06,  1.9426e-06,  3.8742e-06,  ...,  4.0872e-06,\n",
      "           4.2096e-06,  3.9379e-06],\n",
      "         ...,\n",
      "         [-8.4158e-06, -8.3039e-06, -7.1176e-06,  ..., -8.1264e-06,\n",
      "          -9.7789e-06, -1.0906e-05],\n",
      "         [-1.3231e-05, -1.3304e-05, -1.2665e-05,  ..., -8.3589e-06,\n",
      "          -6.9550e-06, -8.8455e-06],\n",
      "         [-7.8478e-06, -6.3345e-06, -6.1381e-06,  ..., -2.8959e-06,\n",
      "          -1.9698e-06,  9.7180e-07]],\n",
      "\n",
      "        [[ 1.6537e-05,  1.5529e-05,  1.4923e-05,  ...,  1.5319e-05,\n",
      "           1.5911e-05,  1.6474e-05],\n",
      "         [-1.8897e-06, -1.2077e-06,  1.2266e-06,  ...,  2.1075e-06,\n",
      "           1.6106e-06,  7.5857e-07],\n",
      "         [ 2.1455e-06,  1.9029e-06,  3.8859e-06,  ...,  4.1052e-06,\n",
      "           4.1970e-06,  3.9703e-06],\n",
      "         ...,\n",
      "         [-8.4149e-06, -8.2975e-06, -7.1170e-06,  ..., -8.1262e-06,\n",
      "          -9.7579e-06, -1.0905e-05],\n",
      "         [-1.3256e-05, -1.3338e-05, -1.2638e-05,  ..., -8.3429e-06,\n",
      "          -6.9455e-06, -8.8435e-06],\n",
      "         [-7.8316e-06, -6.3320e-06, -6.1502e-06,  ..., -2.8736e-06,\n",
      "          -1.9570e-06,  1.0147e-06]]], device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder(bs=batch_size).cuda()\n",
    "#model.load_state_dict(torch.load(MODEL_PATH))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "model.train()\n",
    "\n",
    "# data = next(iter(dataloader))\n",
    "for epoch in trange(num_epochs):\n",
    "    print(f'Starting epoch {epoch + 1 + last_epoch}')\n",
    "    \n",
    "    pbar = tqdm()\n",
    "    pbar.reset(total=(len(dataset) // batch_size))\n",
    "    \n",
    "    for i, data in enumerate(dataloader):\n",
    "        inp = Variable(data[0]).cuda()\n",
    "        output = model(inp)\n",
    "        loss = criterion(output, Variable(data[1]).cuda())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.update()\n",
    "    \n",
    "    pbar.refresh()\n",
    "    \n",
    "    torch.save(model.state_dict(), f'models/noisy_seae_epoch_{epoch + last_epoch + 1}.pth')\n",
    "    \n",
    "    if True or epoch % 5 == 0:\n",
    "        print(f'epoch [{epoch}/{num_epochs}]')\n",
    "\n",
    "        print(round(loss.item(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Autoencoder(bs=batch_size).cuda()\n",
    "model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (in_dropout): Dropout(p=0.3, inplace=False)\n",
       "  (conv_1): Conv1d(1, 16, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_1): PReLU(num_parameters=1)\n",
       "  (conv_2): Conv1d(16, 32, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_2): PReLU(num_parameters=1)\n",
       "  (conv_3): Conv1d(32, 32, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_3): PReLU(num_parameters=1)\n",
       "  (conv_4): Conv1d(32, 64, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_4): PReLU(num_parameters=1)\n",
       "  (conv_5): Conv1d(64, 64, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_5): PReLU(num_parameters=1)\n",
       "  (conv_6): Conv1d(64, 128, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_6): PReLU(num_parameters=1)\n",
       "  (conv_7): Conv1d(128, 128, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_7): PReLU(num_parameters=1)\n",
       "  (conv_8): Conv1d(128, 256, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_8): PReLU(num_parameters=1)\n",
       "  (conv_9): Conv1d(256, 256, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_9): PReLU(num_parameters=1)\n",
       "  (conv_10): Conv1d(256, 512, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_10): PReLU(num_parameters=1)\n",
       "  (conv_11): Conv1d(512, 1024, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_11): PReLU(num_parameters=1)\n",
       "  (deconv_11): ConvTranspose1d(1024, 512, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_d_11): PReLU(num_parameters=1)\n",
       "  (deconv_10): ConvTranspose1d(1024, 256, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_d_10): PReLU(num_parameters=1)\n",
       "  (deconv_9): ConvTranspose1d(512, 256, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_d_9): PReLU(num_parameters=1)\n",
       "  (deconv_8): ConvTranspose1d(512, 128, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_d_8): PReLU(num_parameters=1)\n",
       "  (deconv_7): ConvTranspose1d(256, 128, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_d_7): PReLU(num_parameters=1)\n",
       "  (deconv_6): ConvTranspose1d(256, 64, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_d_6): PReLU(num_parameters=1)\n",
       "  (deconv_5): ConvTranspose1d(128, 64, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_d_5): PReLU(num_parameters=1)\n",
       "  (deconv_4): ConvTranspose1d(128, 32, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_d_4): PReLU(num_parameters=1)\n",
       "  (deconv_3): ConvTranspose1d(64, 32, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_d_3): PReLU(num_parameters=1)\n",
       "  (deconv_2): ConvTranspose1d(64, 16, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_d_2): PReLU(num_parameters=1)\n",
       "  (deconv_1): ConvTranspose1d(32, 1, kernel_size=(32,), stride=(2,), padding=(15,))\n",
       "  (norm_d_1): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional input for 3-dimensional weight 16 1 32, but got 2-dimensional input of size [1, 16384] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-84f5d5371b69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_s\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/www/sound/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mc_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mc_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mc_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    201\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 202\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3-dimensional input for 3-dimensional weight 16 1 32, but got 2-dimensional input of size [1, 16384] instead"
     ]
    }
   ],
   "source": [
    "pesqs = []\n",
    "\n",
    "data = next(iter(dataloader))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample = data[1].cuda()\n",
    "    \n",
    "    for i, _s in enumerate(sample[:10]):\n",
    "        output = model(data[0].cuda())\n",
    "        ref = output[i, :, :].cpu().detach().numpy().T[:, 0]\n",
    "        target = sample[i, :, :].cpu().detach().numpy().T[:, 0]\n",
    "        # plt.figure()\n",
    "        # plt.plot(ref)\n",
    "        # plt.figure()\n",
    "        # plt.plot(target)\n",
    "\n",
    "        pesqs.append(pesq(target, ref, 16000))\n",
    "        \n",
    "print(round(sum(pesqs) / len(pesqs), 4))\n",
    "Audio(ref, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'data/clean/open_slr/2902-9006-0001.wav'\n",
    "wave, _ = torchaudio.load(f)\n",
    "specgram = torchaudio.transforms.Spectrogram(1024, 300)(wave)\n",
    "\n",
    "print(\"Shape of spectrogram: {}\".format(specgram.size()))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(specgram.log2()[0,:,:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
