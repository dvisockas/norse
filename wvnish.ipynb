{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEGAN + WaveNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* ~~Encode wave with mu law~~\n",
    "* ~~Decode wave before reconstruction~~\n",
    "* Predict both noise and clean speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import re\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio import transforms\n",
    "from data import SpeechDataset\n",
    "import time\n",
    "from wavenetish_model import Wavenetish\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from pypesq import pesq\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.set_audio_backend('sox_io')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is ready!\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.cudnn.enabled and torch.cuda.is_available():\n",
    "    print('CUDA is ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 150\n",
    "batch_size = 1024\n",
    "learning_rate = 2e-3\n",
    "window_size = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preload_model_from_weights = False\n",
    "overfit_one_batch = True\n",
    "limit_samples = batch_size if overfit_one_batch else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if preload_model_from_weights:\n",
    "    epochs = filter(lambda x: re.search(\"^seae_epoch_\\d+\\.pth$\", x), os.listdir('models'))\n",
    "    epochs = map(lambda x: int(re.search(\"^seae_epoch_(\\d+)\\.pth$\", x)[1]), epochs)\n",
    "    last_epoch = max(epochs)\n",
    "else:\n",
    "    last_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if int(last_epoch) > 0: MODEL_PATH = f'../models/seae_epoch_{last_epoch}.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to run the training loop, download the OpenSLR12 dataset (http://www.openslr.org/12/), convert all .flac files to .wav and copy to 'data/clean/open_slr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpeechDataset(clean_dir='data/clean/360/',\n",
    "                        noise_dir='data/noise/', \n",
    "                        window_size=window_size, \n",
    "                        overlap=50,\n",
    "                        snr=5, \n",
    "                        limit_samples=batch_size,\n",
    "                        output_one=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6079ddcc7f44c7a30a2044b517b972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc576d74842405593c4310369acde07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=150.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    }
   ],
   "source": [
    "model = Wavenetish(bs=batch_size, pay_attention=False).cuda()\n",
    "\n",
    "if preload_model_from_weights:\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    last_epoch = 0\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#, weight_decay=1e-5)\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "model.train()\n",
    "\n",
    "pbar = tqdm()\n",
    "pbar.reset(total=(len(dataset) // batch_size))\n",
    "    \n",
    "for epoch in trange(num_epochs):\n",
    "    print_epoch = epoch % 10 == 0\n",
    "    save_state = print_epoch\n",
    "    \n",
    "    if print_epoch: print(f'Starting epoch {epoch + 1 + last_epoch}')\n",
    "    \n",
    "    for i, data in enumerate(dataloader):\n",
    "        expected = data[1].cuda()\n",
    "        output = model(data[0].cuda())\n",
    "        loss = criterion(output, expected.reshape(batch_size))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    #pbar.update()    \n",
    "    \n",
    "    pbar.refresh()\n",
    "    if save_state:\n",
    "        pass\n",
    "        #torch.save(model.state_dict(), f'models/noisy_seae_epoch_{epoch + last_epoch + 1}.pth')\n",
    "    \n",
    "    if print_epoch:\n",
    "        print(f'epoch [{epoch}/{num_epochs}]')\n",
    "        print(round(loss.item(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(inp[0].cpu(), rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking (listening) to the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Wavenetish:\n\tUnexpected key(s) in state_dict: \"attn_f.weight\", \"attn_f.bias\", \"attn_g.weight\", \"attn_g.bias\", \"attn_h.weight\", \"attn_h.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b1863021bad0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWavenetish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpay_attention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/overfit_wvn.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1045\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Wavenetish:\n\tUnexpected key(s) in state_dict: \"attn_f.weight\", \"attn_f.bias\", \"attn_g.weight\", \"attn_g.bias\", \"attn_h.weight\", \"attn_h.bias\". "
     ]
    }
   ],
   "source": [
    "model = Wavenetish(bs=batch_size, pay_attention=False).cuda()\n",
    "model.load_state_dict(torch.load('models/overfit_wvn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_file = 'data/clean/360/2156-82458-0026.wav'\n",
    "noise_file = 'data/noise/219164.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "snr = 5\n",
    "noise_wave = torchaudio.load(noise_file)[0]\n",
    "clean_wave = torchaudio.load(clean_file)[0]\n",
    "\n",
    "noise_len = len(noise_wave[0, :])\n",
    "clean_len = len(clean_wave[0, :])\n",
    "\n",
    "if noise_len < clean_len:\n",
    "    repeat_times = math.ceil(clean_len / noise_len)\n",
    "    noise_wave = noise_wave.repeat((1, repeat_times))\n",
    "\n",
    "noised_wave = torch.add(clean_wave[0, :], noise_wave[0, :clean_len] / snr).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_n = 25000\n",
    "\n",
    "nw = noise_wave[0, :first_n].reshape(-1)\n",
    "wv = torchaudio.transforms.MuLawEncoding()(nw)\n",
    "\n",
    "cw = clean_wave[0, :first_n].reshape(-1)\n",
    "cwave = torchaudio.transforms.MuLawEncoding()(cw)\n",
    "\n",
    "deml = torchaudio.transforms.MuLawDecoding()(wv)\n",
    "xs = range(len(deml))\n",
    "\n",
    "fig, axs = plt.subplots(5,figsize=(15,15))\n",
    "axs[0].plot(wv)\n",
    "axs[0].set_title('Mu Law encoded wave')\n",
    "\n",
    "axs[4].plot(nw)\n",
    "axs[4].set_title('Raw wave')\n",
    "\n",
    "axs[2].set_title('Decoded wave')\n",
    "axs[2].plot(deml)\n",
    "\n",
    "axs[3].set_title('Overlap between raw and decoded waves')\n",
    "axs[3].plot(xs, deml, xs, nw)\n",
    "\n",
    "axs[1].set_title('Encoded clean signal')\n",
    "axs[1].plot(cwave)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(noised_wave, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.nn.Softmax()(model(data[0][:100, :, :].cuda())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import windows\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    noise_inputs = windows(noised_wave, window_size, 50, step=1)\n",
    "\n",
    "    predicts = []\n",
    "\n",
    "    num_batches = (len(noised_wave[0]) // batch_size) - 40\n",
    "    print(f'batches: {num_batches}')\n",
    "    for i in range(0, num_batches):\n",
    "        sample = noise_inputs[0, i * batch_size:((i + 1)*batch_size)]\n",
    "        reshaped = sample.reshape(-1, 1, window_size).cuda()\n",
    "        outputs = model(reshaped).detach().cpu()\n",
    "        if i % 20 == 0: print(f'batch: {i}')\n",
    "        for output in outputs:\n",
    "            predicts.append(torch.argmax(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torchaudio.transforms.MuLawDecoding()(torch.tensor(predicts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = []\n",
    "for sample in noise_inputs[0, 16000:32000]:\n",
    "    gts.append(sample[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, len(gts)), gts, range(0, len(predicts)), [predicts * 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = [float(i) for i in predicts]\n",
    "Audio(predicts, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesqs = []\n",
    "\n",
    "data = next(iter(dataloader))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample = data[1].cuda()\n",
    "    inp = data[0]\n",
    "    \n",
    "    for i, _s in enumerate(sample[:50]):\n",
    "        output = model(data[0].cuda())\n",
    "        ref = output[i, :, :].cpu().detach().numpy().T[:, 0]\n",
    "        target = sample[i, :, :].cpu().detach().numpy().T[:, 0]\n",
    "        noised = inp[i, :, :].cpu().detach().numpy().T[:, 0]\n",
    "        \n",
    "        pesqs.append(pesq(target, ref, 16000))\n",
    "        \n",
    "print(round(sum(pesqs) / len(pesqs), 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
