{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train SEAE with Fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from data import SpeechDataset\n",
    "import time\n",
    "from model import Autoencoder\n",
    "from transformer import Transformer\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "from pypesq import pesq\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from IPython.display import Audio\n",
    "from fastai.vision import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLORE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpeechDataset('data/clean/360/', 'data/noise/', window_size=16384, overlap=50, snr=10)\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "if EXPLORE:\n",
    "    dataset_size = batch_size * 3\n",
    "indices = list(range(dataset_size))\n",
    "validation_split = .2\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "validation_loader = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(bs=batch_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of https://arxiv.org/pdf/1903.03107v1.pdf \n",
    "def weightedSDR(output, target):\n",
    "    output = output.view(-1, 16384)\n",
    "    target = target.view(-1, 16384)\n",
    "    \n",
    "    dot_product = torch.sum(output * target)\n",
    "    loss = (-1 * dot_product) / (torch.norm(target) * torch.norm(output))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(train_loader, validation_loader)\n",
    "learner = Learner(data, model, opt_func=torch.optim.RMSprop, loss_func=root_mean_squared_error, callback_fns=ShowGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(20, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_one_cycle(100, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'models/attn_3_cycle.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's hear how the model denoises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypesq import pesq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesqs = []\n",
    "\n",
    "data = dataset[0]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample = data[0].cuda()\n",
    "    output = model(data[0].reshape(-1, 1, 16384).cuda())\n",
    "    ref = output[0, :, :].cpu().detach().numpy().T[:, 0]\n",
    "    target = data[1][0, :].cpu().detach().numpy().T[:]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(ref)\n",
    "    plt.figure()\n",
    "    plt.plot(target)\n",
    "\n",
    "    pesqs.append(pesq(target, ref, 16000))\n",
    "        \n",
    "print(round(sum(pesqs) / len(pesqs), 4))\n",
    "# Change between ref/target to hear model output/original\n",
    "Audio(ref, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = dataset[0][0].reshape(1,1, 16384)\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=16384, nhead=4)\n",
    "#src = torch.rand(10, 32, 512)\n",
    "out = encoder_layer(src)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
